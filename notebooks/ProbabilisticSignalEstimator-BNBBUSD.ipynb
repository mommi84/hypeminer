{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "mathematical-honor",
   "metadata": {},
   "source": [
    "# Probabilistic Signal Estimator\n",
    "\n",
    "The question we want to answer is the following. How long does it take to reach +2% gain from now?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "floating-victorian",
   "metadata": {},
   "outputs": [],
   "source": [
    "from hypecommons import *\n",
    "from hypecommons import plot as hyplot\n",
    "\n",
    "from datetime import datetime, timedelta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "liable-dodge",
   "metadata": {},
   "outputs": [],
   "source": [
    "CRYPTO = 'BNB'\n",
    "FIAT = 'BUSD'\n",
    "SYMBOL = f\"{CRYPTO}{FIAT}\"\n",
    "FREQ = 1\n",
    "START = '20210101000000'\n",
    "TARGET = 1.02\n",
    "DAYS = 150"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "junior-choice",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<center><img style='display: flex;' src='../img/symbols/BNB.png'></center>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.display import display, HTML\n",
    "display(HTML(f\"<center><img style='display: flex;' src='../img/symbols/{CRYPTO}.png'></center>\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "clean-henry",
   "metadata": {},
   "outputs": [
    {
     "ename": "OSError",
     "evalue": "[Errno 22] Invalid argument",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-4-6a8430374fd4>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mdf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdownload_history_fast\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mSYMBOL\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mSTART\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfreq\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mFREQ\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdays\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mDAYS\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\Workspace\\hypeminer\\notebooks\\hypecommons.py\u001b[0m in \u001b[0;36mdownload_history_fast\u001b[1;34m(symbol, start, freq, days, cache)\u001b[0m\n\u001b[0;32m    123\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mcache\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0misfile\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfile_dest\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    124\u001b[0m \u001b[1;31m#         print(f\"File {file_dest} found.\")\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 125\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mload_file\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfile_dest\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    126\u001b[0m     \u001b[0mepoch\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mto_epoch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstart\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    127\u001b[0m     \u001b[0mepoch_at_start\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mepoch\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Workspace\\hypeminer\\notebooks\\hypecommons.py\u001b[0m in \u001b[0;36mload_file\u001b[1;34m(file)\u001b[0m\n\u001b[0;32m     37\u001b[0m     \u001b[1;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfile\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     38\u001b[0m         \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mjson\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 39\u001b[1;33m     \u001b[0mindex\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mdatetime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfromtimestamp\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mv\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'epoch'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstrftime\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'%Y-%m-%d %H:%M:%S'\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mv\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     40\u001b[0m     \u001b[0mdf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mindex\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     41\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mfield\u001b[0m \u001b[1;32min\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;34m'open'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'high'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'low'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'close'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'volume'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'trades'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Workspace\\hypeminer\\notebooks\\hypecommons.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m     37\u001b[0m     \u001b[1;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfile\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     38\u001b[0m         \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mjson\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 39\u001b[1;33m     \u001b[0mindex\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mdatetime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfromtimestamp\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mv\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'epoch'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstrftime\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'%Y-%m-%d %H:%M:%S'\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mv\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     40\u001b[0m     \u001b[0mdf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mindex\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     41\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mfield\u001b[0m \u001b[1;32min\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;34m'open'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'high'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'low'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'close'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'volume'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'trades'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mOSError\u001b[0m: [Errno 22] Invalid argument"
     ]
    }
   ],
   "source": [
    "df = download_history_fast(SYMBOL, START, freq=FREQ, days=DAYS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "loved-florence",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(str(df.iloc[0]) + '\\n\\n' + str(df.iloc[-1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "earlier-direction",
   "metadata": {},
   "source": [
    "As the goal is to achieve a given value, the `high` price value is used. Binance allows users to create future orders with a limit, therefore this behaviour can be reproduced also in a real-time setting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "impressed-finland",
   "metadata": {},
   "outputs": [],
   "source": [
    "def minutes_to_target(df, start, target):\n",
    "    t0 = df.index[start]\n",
    "    close0 = df['close'].iloc[start]\n",
    "    try:\n",
    "        t1 = df[(df.index > t0) & (df['high'] >= target * close0)].index[0]\n",
    "    except IndexError:\n",
    "        return None, None # it never achieves the target\n",
    "    return t1, round((t1 - t0).total_seconds() / 60.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "directed-humor",
   "metadata": {},
   "outputs": [],
   "source": [
    "x0 = minutes_to_target(df, 0, target=TARGET)\n",
    "print(x0)\n",
    "df.iloc[int(x0[1]/FREQ)]['high'] / df.iloc[0]['close']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "retired-essex",
   "metadata": {},
   "outputs": [],
   "source": [
    "mins2tgt = []\n",
    "\n",
    "rng = range(len(df))[::100]\n",
    "\n",
    "for i in tqdm(rng, desc='Testing minutes to target...'):\n",
    "    t, y = minutes_to_target(df, i, target=TARGET)\n",
    "    if y:\n",
    "        mins2tgt.append(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "assured-living",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_m = pd.DataFrame(mins2tgt)\n",
    "pos, total = len(df_m[df_m[0] <= 600]), len(df_m)\n",
    "print(f\"In {pos} cases out of {total} (~{pos/total*100:.0f}%), 2% gain is reached after less than 10 hours.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "pleasant-queensland",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.rcParams.update({'figure.figsize': (7,5), 'figure.dpi': 100})\n",
    "\n",
    "n_bins = 200\n",
    "hist_x, hist_y, hist_z = plt.hist(mins2tgt, bins=n_bins)\n",
    "\n",
    "print(f\"First 10 bins: {hist_x[:10]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "lovely-training",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Each bin represents around 60000/{n_bins} = {60000/n_bins} minutes = {60000/n_bins/60} hours of time,\\n\"\n",
    "      f\"which means {sum(hist_x[:1])/len(mins2tgt)*100:.1f}% of the time we reach +2% gain within {60000/n_bins/60} hours,\\n\"\n",
    "      f\"{sum(hist_x[:2])/len(mins2tgt)*100:.1f}% of the time within {2*60000/n_bins/60} hours, and so on.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b4e2764",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ema(data, n):\n",
    "    alpha = 2 / (1 + n)\n",
    "    return data.ewm(alpha=alpha, adjust=False).mean()\n",
    "\n",
    "def normalise(df_orig):\n",
    "    df = df_orig.copy()\n",
    "    \n",
    "    fields = list(df)\n",
    "    \n",
    "    for field in fields:\n",
    "        if field in ['volume', 'trades']:\n",
    "            for ma in [1, 3, 9]:\n",
    "                df[f\"{field}_pm_ma{ma}\"] = df[field].rolling(window=ma).mean() / FREQ\n",
    "        else:\n",
    "            if field != 'close':\n",
    "                df[f\"{field}_norm\"] = df[field] / df['close']\n",
    "\n",
    "#         if field != 'close':\n",
    "#             df.drop(field, axis=1, inplace=True)\n",
    "    \n",
    "    for x in [50, 200]:\n",
    "        df[f\"close_ma{x}_norm\"] = df['close'].rolling(window=x).mean() / df['close']\n",
    "    \n",
    "    for x in [12, 26]:\n",
    "        df[f\"close_ema{x}_norm\"] = ema(df['close'], x) / df['close']\n",
    "    \n",
    "#     df.drop('close', axis=1, inplace=True)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "retained-creativity",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_n = normalise(df)\n",
    "df_n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "alone-hotel",
   "metadata": {},
   "outputs": [],
   "source": [
    "from joblib import Parallel, delayed  \n",
    "\n",
    "def compute_mins(i):\n",
    "    t, y = minutes_to_target(df, i, target=TARGET)\n",
    "    if y:\n",
    "        return y\n",
    "    else:\n",
    "        return 120000\n",
    "\n",
    "mins2tgt = Parallel(n_jobs=16)(delayed(compute_mins)(i) for i in tqdm(range(len(df))))\n",
    "\n",
    "df_n['mins2tgt'] = mins2tgt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "played-thomas",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv(f\"{SYMBOL}-{START}-{FREQ}-{DAYS}-probabilistic-raw-{TARGET}.csv\")\n",
    "df_n.to_csv(f\"{SYMBOL}-{START}-{FREQ}-{DAYS}-probabilistic-norm-{TARGET}.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "nonprofit-shade",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_n.dropna(inplace=True)\n",
    "df_n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "rotary-president",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_n[df_n['mins2tgt'] == 120000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adapted-clothing",
   "metadata": {},
   "outputs": [],
   "source": [
    "signal_cat = ['VERY HIGH', 'HIGH', 'NORMAL', 'LOW', 'VERY LOW']\n",
    "signal_thr = [0.2, 0.4, 0.6, 0.8, 1.0]\n",
    "\n",
    "def find_quantiles(df_q):\n",
    "    quantiles = []\n",
    "    for c, q in zip(signal_cat, signal_thr):\n",
    "        quantiles.append({'cat': c, 'quantile': q, 'value': df_q['mins2tgt'].quantile(q)})\n",
    "    return quantiles\n",
    "\n",
    "quantiles = find_quantiles(df_n)\n",
    "quantiles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "capable-somewhere",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Everything higher than {quantiles[-2]['value']} will be treated as VERY LOW, therefore we can remove\\n\"\n",
    "      f\"the last rows within such interval.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "accomplished-dodge",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_n[df_n.index >= df_n.index[-1]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "solved-check",
   "metadata": {},
   "outputs": [],
   "source": [
    "remove_after_this = df_n.index[-1] - timedelta(minutes=quantiles[-2]['value'])\n",
    "df_ml = df_n[df_n.index < remove_after_this].copy()\n",
    "df_ml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fifteen-cooler",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ml.to_csv(f\"{SYMBOL}-{START}-{FREQ}-{DAYS}-probabilistic-ml-{TARGET}.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e6e9924",
   "metadata": {},
   "source": [
    "## Train model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "senior-shoulder",
   "metadata": {},
   "source": [
    "A few data visualisations..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cathedral-cookbook",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "plot(plt.plot, df_n.iloc[2000:4000], ['mins2tgt'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "built-opera",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot(plt.plot, df.dropna().iloc[2000:4000], ['high'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "coordinated-yellow",
   "metadata": {},
   "outputs": [],
   "source": [
    "# p = 3009\n",
    "# print(df.dropna().iloc[p])\n",
    "\n",
    "# print(f\"\\ntarget: {df.dropna().iloc[p]['close'] * 1.02}\")\n",
    "\n",
    "# df[df.index == df.dropna().index[p] + timedelta(minutes=df.dropna().iloc[p]['mins2tgt'])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "sticky-apollo",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['mins2tgt'] = df_n['mins2tgt']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "proud-mountain",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "specific-services",
   "metadata": {},
   "source": [
    "## Load from file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc13e9c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from hypecommons import *\n",
    "from hypecommons import plot as hyplot\n",
    "\n",
    "from datetime import datetime, timedelta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "trained-dance",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "SYMBOL = 'BNBBUSD'\n",
    "FREQ = 1\n",
    "START = '20210101000000'\n",
    "TARGET = 1.02\n",
    "DAYS = 150\n",
    "\n",
    "df = download_history_fast(SYMBOL, START, freq=FREQ, days=DAYS)\n",
    "\n",
    "df_ml = pd.read_csv(f\"{SYMBOL}-{START}-{FREQ}-{DAYS}-probabilistic-ml-{TARGET}.csv\").set_index('Unnamed: 0')\n",
    "df_ml = df_ml.set_index(pd.to_datetime(df_ml.index))\n",
    "df_ml"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "381d161a",
   "metadata": {},
   "source": [
    "### TODO this is where it should be"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "connected-pregnancy",
   "metadata": {},
   "outputs": [],
   "source": [
    "signal_cat = ['VERY HIGH', 'HIGH', 'NORMAL', 'LOW', 'VERY LOW']\n",
    "signal_thr = [0.2, 0.4, 0.6, 0.8, 1.0]\n",
    "\n",
    "def find_quantiles(df_q):\n",
    "    quantiles = []\n",
    "    for c, q in zip(signal_cat, signal_thr):\n",
    "        quantiles.append({'cat': c, 'quantile': q, 'value': df_q['mins2tgt'].quantile(q)})\n",
    "    return quantiles\n",
    "\n",
    "quantiles = find_quantiles(df_ml)\n",
    "quantiles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "wound-trace",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ml['cat'] = None\n",
    "\n",
    "def to_cat(x):\n",
    "    for q in reversed(quantiles):\n",
    "        if x <= q['value']:\n",
    "            res = q['cat']\n",
    "    return res\n",
    "\n",
    "df_ml['cat'] = df_ml['mins2tgt'].map(to_cat)\n",
    "df_ml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "surface-crazy",
   "metadata": {},
   "outputs": [],
   "source": [
    "str(list(df_ml.columns))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "turned-mercury",
   "metadata": {},
   "outputs": [],
   "source": [
    "feat_cols = ['open_norm', 'high_norm', 'low_norm', 'volume_pm_ma1', 'volume_pm_ma3', \n",
    "             'volume_pm_ma9', 'trades_pm_ma1', 'trades_pm_ma3', 'trades_pm_ma9', \n",
    "             'close_ma50_norm', 'close_ma200_norm', 'close_ema12_norm', 'close_ema26_norm']\n",
    "df_ml_feats = df_ml[feat_cols]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "assumed-fellow",
   "metadata": {},
   "source": [
    "## Dimensionality reduction and visualisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "reflected-mainstream",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# # apply dimensionality reduction\n",
    "\n",
    "# # from sklearn.decomposition import PCA\n",
    "# # pca = PCA(n_components=2)\n",
    "# # dimred_result = pca.fit_transform(df_ml_feats)\n",
    "\n",
    "# from sklearn.manifold import TSNE\n",
    "# tsne = TSNE(n_components=2, verbose=1, perplexity=40, n_iter=250)\n",
    "# dimred_result = tsne.fit_transform(df_ml_feats)\n",
    "\n",
    "# import numpy as np\n",
    "# import math\n",
    "# import matplotlib.pyplot as plt\n",
    "\n",
    "# plt.figure(figsize=(16,10))\n",
    "\n",
    "# to_colour = {}\n",
    "# for q in quantiles:\n",
    "#     to_colour[q['cat']] = len(to_colour)\n",
    "\n",
    "# colours = [to_colour[x] for x in list(df_ml['cat'])]\n",
    "\n",
    "# plt.scatter(dimred_result[:,0], dimred_result[:,1], c=colours)\n",
    "# plt.legend(loc=\"upper left\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "formed-combat",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(df_ml_feats)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "expressed-salad",
   "metadata": {},
   "source": [
    "## Multiclass model training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "legendary-series",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import numpy as np\n",
    "# from sklearn.pipeline import make_pipeline\n",
    "# from sklearn.preprocessing import StandardScaler\n",
    "# from sklearn.svm import SVC\n",
    "\n",
    "# TRAINING_SIZE = 100000\n",
    "\n",
    "# clf = make_pipeline(StandardScaler(), SVC(gamma='auto'))\n",
    "# clf.fit(df_ml_feats[:TRAINING_SIZE], df_ml['cat'][:TRAINING_SIZE])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "concrete-enzyme",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# from sklearn.metrics import confusion_matrix, precision_recall_fscore_support\n",
    "\n",
    "# def evaluate():\n",
    "#     # evaluate on last N examples\n",
    "#     N = 20000\n",
    "#     labels = list(reversed(signal_cat))\n",
    "#     test = df_ml_feats.tail(N)\n",
    "#     actual = df_ml['cat'].tail(N)\n",
    "#     pred = clf.predict(test)\n",
    "#     cm = confusion_matrix(actual, pred, labels=labels)\n",
    "#     prf = precision_recall_fscore_support(actual, pred, labels=labels)\n",
    "#     print(f\"{cm}\\n\")\n",
    "#     T = 10\n",
    "#     print(\"          \\t\" + \"\\t\".join(labels).expandtabs(T))\n",
    "#     print(\"Precision:\\t\" + \"\\t\".join([f\"{x:.3f}\" for x in prf[0]]).expandtabs(T))\n",
    "#     print(\"Recall:   \\t\" + \"\\t\".join([f\"{x:.3f}\" for x in prf[1]]).expandtabs(T))\n",
    "#     print(\"F-Score:  \\t\" + \"\\t\".join([f\"{x:.3f}\" for x in prf[2]]).expandtabs(T))\n",
    "\n",
    "# evaluate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "accurate-prospect",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_ml_feats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "completed-training",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_ml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "quiet-christopher",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_plot"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acknowledged-flooring",
   "metadata": {},
   "source": [
    "## Signal detection\n",
    "\n",
    "Each dot is a moment between Jan 1st and June 1st this year. The blue dots are moments with a high buy signal where the price went +2% in less than 30 hours (`VERY HIGH`, `HIGH`, `NORMAL`, and `LOW` categories). My aim is to detect them automatically."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "specified-imperial",
   "metadata": {},
   "outputs": [],
   "source": [
    "# filter out y=120k (which means 'never reaches +2%') and y>40k (exceptionally bad points)\n",
    "# df_plot = df_ml[df_ml['mins2tgt'] < 40000].copy()\n",
    "df_plot = df_ml.copy()\n",
    "\n",
    "good_signals = ['VERY HIGH', 'HIGH', 'NORMAL', 'LOW']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "liked-replica",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "for fc in feat_cols:\n",
    "    print(fc)\n",
    "    # red dots = idle signals\n",
    "    plt.scatter(df_plot[~df_plot['cat'].isin(good_signals)][fc], \n",
    "                df_plot[~df_plot['cat'].isin(good_signals)]['mins2tgt'])\n",
    "    # blue dots = buy signals\n",
    "    plt.scatter(df_plot[df_plot['cat'].isin(good_signals)][fc], \n",
    "                df_plot[df_plot['cat'].isin(good_signals)]['mins2tgt'])\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "027203cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_plot = df_plot.join(df[['close']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1507de9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def rsi(data, n):\n",
    "    # Make the positive gains (up) and negative gains (down) Series\n",
    "    delta = data.diff()\n",
    "    up, down = delta.copy(), delta.copy()\n",
    "    up[up < 0] = 0\n",
    "    down[down > 0] = 0\n",
    "\n",
    "    # Calculate the EWMA\n",
    "    roll_up1 = up.ewm(span=n).mean()\n",
    "    roll_down1 = down.abs().ewm(span=n).mean()\n",
    "\n",
    "    # Calculate the RSI based on EWMA\n",
    "    rs1 = roll_up1 / roll_down1\n",
    "    return 100.0 - (100.0 / (1.0 + rs1))\n",
    "\n",
    "df_plot['rsi14'] = rsi(df_plot['close'], 14)\n",
    "\n",
    "for fc in ['rsi14']:\n",
    "    print(fc)\n",
    "    # red dots = idle signals\n",
    "    plt.scatter(df_plot[~df_plot['cat'].isin(good_signals)][fc], \n",
    "                df_plot[~df_plot['cat'].isin(good_signals)]['mins2tgt'])\n",
    "    # blue dots = buy signals\n",
    "    plt.scatter(df_plot[df_plot['cat'].isin(good_signals)][fc], \n",
    "                df_plot[df_plot['cat'].isin(good_signals)]['mins2tgt'])\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "standing-description",
   "metadata": {},
   "source": [
    "Check what happened right before the rightmost blue moments in the chart."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "directed-gentleman",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# for t0 in df_ml[df_ml['close_ema26_norm'] > 1.05].index:\n",
    "#     print(f\"{df.loc[t0]}\\n\")\n",
    "#     plot(plt.plot, df[df.index <= t0].tail(26), ['close', 'high'])\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "absent-parameter",
   "metadata": {},
   "source": [
    "## Threshold study"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "verbal-suite",
   "metadata": {},
   "outputs": [],
   "source": [
    "# relevants = len(df_ml['cat'].isin(good_signals))\n",
    "\n",
    "def threshold_study(field, val_min, val_max, target):\n",
    "    thrs = np.arange(val_min, val_max, 0.005)\n",
    "    precs = []\n",
    "    for thr in thrs:\n",
    "        goods = len(df_ml[(df_ml[field] >= thr) & (df_ml['cat'].isin(good_signals))])\n",
    "        bads = len(df_ml[(df_ml[field] >= thr) & (~df_ml['cat'].isin(good_signals))])\n",
    "        prec = goods / (goods + bads) if goods + bads > 0 else 0\n",
    "#         rec = goods / relevants\n",
    "        precs.append(prec if target == 'p' else goods / DAYS)\n",
    "    \n",
    "    plt.plot(thrs, precs, label=field)\n",
    "    if target == 's':\n",
    "        plt.yscale('log')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "asian-earthquake",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "threshold_study('close_ema26_norm', 1.01, 1.2, 'p')\n",
    "threshold_study('close_ma200_norm', 1.01, 1.2, 'p')\n",
    "plt.legend(loc=\"best\")\n",
    "plt.title(f\"Precision for each field\")\n",
    "plt.show()\n",
    "\n",
    "threshold_study('close_ema26_norm', 1.01, 1.2, 's')\n",
    "threshold_study('close_ma200_norm', 1.01, 1.2, 's')\n",
    "plt.legend(loc=\"best\")\n",
    "plt.title(f\"Support per day for each field\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "still-biography",
   "metadata": {},
   "source": [
    "## Binary model training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "brazilian-studio",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ml_feats_subset = df_ml_feats[['close_ema26_norm', 'close_ma200_norm']].copy()\n",
    "df_ml_feats_binary = pd.DataFrame(df_ml['cat'].isin(good_signals))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "focal-arrangement",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import numpy as np\n",
    "# from sklearn.pipeline import make_pipeline\n",
    "# from sklearn.preprocessing import StandardScaler\n",
    "# from sklearn.svm import SVC\n",
    "\n",
    "# N = 100000\n",
    "# clf = make_pipeline(StandardScaler(), SVC(gamma='auto'))\n",
    "# clf.fit(df_ml_feats_subset.head(N), df_ml_feats_binary.head(N))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "comparative-moisture",
   "metadata": {},
   "outputs": [],
   "source": [
    "# x_min, x_max = 0.5, 1.5\n",
    "# y_min, y_max = 0.5, 1.5\n",
    "# h = .005  # step size in the mesh\n",
    "# xx, yy = np.meshgrid(np.arange(x_min, x_max, h),\n",
    "#                      np.arange(y_min, y_max, h))\n",
    "\n",
    "# # Plot the decision boundary. For that, we will assign a color to each\n",
    "# # point in the mesh [x_min, x_max]x[y_min, y_max].\n",
    "# Z = clf.predict(np.c_[xx.ravel(), yy.ravel()])\n",
    "\n",
    "# # Put the result into a color plot\n",
    "# Z = Z.reshape(xx.shape)\n",
    "# plt.figure(figsize=(16,10))\n",
    "# plt.contourf(xx, yy, 1-Z, cmap=plt.cm.coolwarm)\n",
    "\n",
    "# #     # Plot also the training points\n",
    "# #     plt.scatter(X[:, 0], X[:, 1], c=y, cmap=plt.cm.coolwarm)\n",
    "# plt.xlabel('close_ema26_norm')\n",
    "# plt.ylabel('close_ma200_norm')\n",
    "# plt.xlim(xx.min(), xx.max())\n",
    "# plt.ylim(yy.min(), yy.max())\n",
    "# plt.title('SVC')\n",
    "\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "amended-metropolitan",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "# manually set thresholds\n",
    "thr_x, thr_y = 1.02, 1.14\n",
    "\n",
    "tp, fp = 0, 0\n",
    "\n",
    "true_positive = []\n",
    "\n",
    "for (_, X), (_, y) in tqdm(zip(df_ml_feats_subset.iterrows(), df_ml_feats_binary.iterrows()), \n",
    "                           desc='Evaluating precision...', total=len(df_ml_feats_subset), ncols=100):\n",
    "#     print(X['close_ema26_norm'], X['close_ma200_norm'], y['cat'])\n",
    "    if X['close_ema26_norm'] >= thr_x or X['close_ma200_norm'] >= thr_y:\n",
    "        if y['cat']:\n",
    "            tp += 1\n",
    "            true_positive.append(True)\n",
    "        else:\n",
    "            fp += 1\n",
    "            true_positive.append(False)\n",
    "    else:\n",
    "        true_positive.append(False)\n",
    "\n",
    "df_ml_feats_subset['tp'] = true_positive\n",
    "df_tp = df_ml_feats_subset[df_ml_feats_subset['tp'] == True].copy()\n",
    "df_tp['date'] = pd.to_datetime(df_tp.index)\n",
    "\n",
    "evaluation = {'pre': tp/(tp+fp), 'tp': tp, 'fp': fp, 'tp/day': tp/DAYS}\n",
    "\n",
    "print(evaluation)\n",
    "\n",
    "c = Counter()\n",
    "\n",
    "for i, row in df_tp.iterrows():\n",
    "    c[(row['date'].year, row['date'].month, row['date'].day)] += 1\n",
    "\n",
    "print(f\"Unique days with prospect of +2%: {len(c)}\")\n",
    "print(f\"Total predicted change: {(1.02 ** len(c) - 1) * evaluation['pre']:+.1%}\")\n",
    "\n",
    "start = pd.to_datetime(df_ml_feats_subset.index[0])\n",
    "end = pd.to_datetime(df_ml_feats_subset.index[-1])\n",
    "\n",
    "t = start\n",
    "datedist = []\n",
    "\n",
    "while True:\n",
    "    datedist.append({'date': datetime(t.year, t.month, t.day), 'tp': c[(t.year, t.month, t.day)]})\n",
    "    t = t + timedelta(days=1)\n",
    "    if t >= end:\n",
    "        break\n",
    "\n",
    "df_datedist = pd.DataFrame.from_records(datedist, index='date')\n",
    "\n",
    "plot(plt.bar, df_datedist, ['tp'], bar_size=.9)\n",
    "plt.show()\n",
    "\n",
    "plot(plt.plot, df[::60], ['close'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "biological-heather",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def is_good_signal(X, Y, decfun=False):\n",
    "#     Tx = -0.809\n",
    "#     Ty = -0.234\n",
    "#     F = (12*(X+Tx) - 3*(Y+Ty))**2 + (X+Tx) + (Y+Ty) - 1\n",
    "#     return F if decfun else F >= 0\n",
    "\n",
    "from math import cos, sin, pi\n",
    "\n",
    "def is_good_signal(X, Y, decfun=False):\n",
    "    A = .012\n",
    "    B = .046\n",
    "    x = X - .998\n",
    "    y = Y - .995\n",
    "    alpha = -pi/14\n",
    "    F = (x * cos(alpha) + y * sin(alpha)) ** 2 / A ** 2 + (x * sin(alpha) - y * cos(alpha)) ** 2 / B ** 2 - 1\n",
    "    return F if decfun else F >= 0\n",
    "\n",
    "plt.figure(figsize=(12, 12))\n",
    "\n",
    "bad_signals = ['VERY LOW']\n",
    "\n",
    "palette = plt.rcParams['axes.prop_cycle'].by_key()['color']  \n",
    "to_col = {1: good_signals, 0: bad_signals}\n",
    "\n",
    "delta = 0.001\n",
    "xy_min, xy_max = 0.9, 1.1\n",
    "xrange = np.arange(xy_min, xy_max, delta)\n",
    "yrange = np.arange(xy_min, xy_max, delta)\n",
    "X, Y = np.meshgrid(xrange,yrange)\n",
    "\n",
    "F = is_good_signal(X, Y, decfun=True)\n",
    "plt.contour(X, Y, F, [0])\n",
    "\n",
    "for c, cl in to_col.items():\n",
    "    selection = df_ml['cat'].isin(cl)\n",
    "    plt.scatter(df_ml_feats[selection]['close_ema26_norm'], \n",
    "                df_ml_feats[selection]['close_ma200_norm'], \n",
    "                s=3, c=palette[c], label=cl)\n",
    "\n",
    "plt.legend(loc=\"upper left\")\n",
    "plt.title(\"close_ema26_norm vs close_ma200_norm\")\n",
    "plt.gca().set_xlim([xy_min, xy_max])\n",
    "plt.gca().set_ylim([xy_min, xy_max])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "binary-recommendation",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "tp, fp = 0, 0\n",
    "\n",
    "true_positive = []\n",
    "\n",
    "for (_, X), (_, y) in tqdm(zip(df_ml_feats_subset.iterrows(), df_ml_feats_binary.iterrows()), \n",
    "                           desc='Evaluating precision...', total=len(df_ml_feats_subset), ncols=100):\n",
    "#     print(X['close_ema26_norm'], X['close_ma200_norm'], y['cat'])\n",
    "    if is_good_signal(X['close_ema26_norm'], X['close_ma200_norm']):\n",
    "        if y['cat']:\n",
    "            tp += 1\n",
    "            true_positive.append(True)\n",
    "        else:\n",
    "            fp += 1\n",
    "            true_positive.append(False)\n",
    "    else:\n",
    "        true_positive.append(False)\n",
    "\n",
    "df_ml_feats_subset['tp'] = true_positive\n",
    "df_tp = df_ml_feats_subset[df_ml_feats_subset['tp'] == True].copy()\n",
    "df_tp['date'] = pd.to_datetime(df_tp.index)\n",
    "\n",
    "evaluation = {'pre': tp/(tp+fp), 'tp': tp, 'fp': fp, 'tp/day': tp/DAYS}\n",
    "\n",
    "print(evaluation)\n",
    "\n",
    "c = Counter()\n",
    "\n",
    "for i, row in df_tp.iterrows():\n",
    "    c[(row['date'].year, row['date'].month, row['date'].day)] += 1\n",
    "\n",
    "print(f\"Unique days with prospect of +2%: {len(c)}\")\n",
    "# print(f\"Total predicted change: {(1.02 ** len(c) - 1) * evaluation['pre']:+.1%}\")\n",
    "\n",
    "start = pd.to_datetime(df_ml_feats_subset.index[0])\n",
    "end = pd.to_datetime(df_ml_feats_subset.index[-1])\n",
    "\n",
    "t = start\n",
    "datedist = []\n",
    "\n",
    "while True:\n",
    "    datedist.append({'date': datetime(t.year, t.month, t.day), 'tp': c[(t.year, t.month, t.day)]})\n",
    "    t = t + timedelta(days=1)\n",
    "    if t >= end:\n",
    "        break\n",
    "\n",
    "df_datedist = pd.DataFrame.from_records(datedist, index='date')\n",
    "\n",
    "plot(plt.bar, df_datedist, ['tp'], bar_size=.9)\n",
    "plt.show()\n",
    "\n",
    "plot(plt.plot, df[::60], ['close'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "atlantic-purpose",
   "metadata": {},
   "source": [
    "## Simulation & stop loss optimisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "medical-bahamas",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from math import cos, sin, pi\n",
    "\n",
    "# def is_good_signal(X, Y, decfun=False):\n",
    "#     Tx = -0.809\n",
    "#     Ty = -0.234\n",
    "#     F = (12*(X+Tx) - 3*(Y+Ty))**2 + (X+Tx) + (Y+Ty) - 1\n",
    "#     return F if decfun else F >= 0\n",
    "\n",
    "def is_good_signal(X, Y, decfun=False):\n",
    "    A = .012\n",
    "    B = .046\n",
    "    x = X - .998\n",
    "    y = Y - .995\n",
    "    alpha = -pi/14\n",
    "    F = (x * cos(alpha) + y * sin(alpha)) ** 2 / A ** 2 + (x * sin(alpha) - y * cos(alpha)) ** 2 / B ** 2 - 1\n",
    "    return F if decfun else F >= 0\n",
    "\n",
    "SYMBOL = 'BNBBUSD'\n",
    "FREQ = 1\n",
    "START = '20210601000000'\n",
    "TARGET = 1.02\n",
    "DAYS = 153\n",
    "# START = '20211001000000'\n",
    "# TARGET = 1.02\n",
    "# DAYS = 23\n",
    "\n",
    "df_sim = download_history_fast(SYMBOL, START, freq=FREQ, days=DAYS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "deluxe-david",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sim.dropna(inplace=True)\n",
    "\n",
    "df_sim = normalise(df_sim)\n",
    "\n",
    "df_sim['pred'] = np.vectorize(is_good_signal)(df_sim['close_ema26_norm'], df_sim['close_ma200_norm'])\n",
    "\n",
    "df_sim = df_sim.set_index(pd.to_datetime(df_sim.index))\n",
    "\n",
    "# ------- simulation -------\n",
    "\n",
    "STOP = 0.97\n",
    "HRS_FROZEN = 0 # 0\n",
    "\n",
    "fees = 0.001\n",
    "stop_loss = 1 * STOP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "baecac71",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sim = simulate(df_sim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "million-drinking",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "df_sim[df_sim['action'].isin(['PROFIT', 'LOSS'])][\n",
    "    ['close', 'close_ema26_norm', 'close_ma200_norm', 'stake', 'action']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "promotional-portland",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "df_sim[df_sim['action'].isin(['PROFIT', 'LOSS'])]['action'].str.get_dummies().sum().plot(\n",
    "    kind='pie', label='action', autopct='%1.0f%%', figsize=[5, 5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "associate-asbestos",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot(plt.plot, df_sim, ['stake'], fig_size=(16, 8))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "incorporate-vanilla",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot(plt.plot, df_sim, ['close'], fig_size=(16, 8))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "complicated-montreal",
   "metadata": {},
   "source": [
    "## Test of time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "loaded-excuse",
   "metadata": {},
   "outputs": [],
   "source": [
    "from math import cos, sin, pi\n",
    "from datetime import timedelta\n",
    "\n",
    "def ema(data, n):\n",
    "    alpha = 2 / (1 + n)\n",
    "    return data.ewm(alpha=alpha, adjust=False).mean()\n",
    "\n",
    "def normalise(df_orig):\n",
    "    df = df_orig.copy()\n",
    "    \n",
    "    fields = list(df)\n",
    "    \n",
    "    for field in fields:\n",
    "        if field in ['volume', 'trades']:\n",
    "            for ma in [1, 3, 9]:\n",
    "                df[f\"{field}_pm_ma{ma}\"] = df[field].rolling(window=ma).mean() / FREQ\n",
    "        else:\n",
    "            if field != 'close':\n",
    "                df[f\"{field}_norm\"] = df[field] / df['close']\n",
    "\n",
    "    for x in [50, 200]:\n",
    "        df[f\"close_ma{x}_norm\"] = df['close'].rolling(window=x).mean() / df['close']\n",
    "    \n",
    "    for x in [12, 26]:\n",
    "        df[f\"close_ema{x}_norm\"] = ema(df['close'], x) / df['close']\n",
    "    \n",
    "    return df\n",
    "\n",
    "def simulate(df_sim, target=1.02, stop=0.94, hrs_frozen=0, fees=0.001):\n",
    "    stake = [1]\n",
    "    action = []\n",
    "    invested = False\n",
    "    frozen = False\n",
    "    for i, row in tqdm(df_sim.iterrows(), desc='Simulating...', ncols=100, total=len(df_sim)):\n",
    "        if frozen and i >= last_loss + timedelta(hours=hrs_frozen):\n",
    "            frozen = False\n",
    "        if frozen and i < last_loss + timedelta(hours=hrs_frozen):\n",
    "            stake.append(stake[-1])\n",
    "            action.append('FROZEN')\n",
    "        elif row['pred'] and not invested:\n",
    "            stake.append(stake[-1] * (1-fees))\n",
    "            invested = True\n",
    "            take_profit = row['close'] * target\n",
    "            stop_loss = row['close'] * stop\n",
    "            action.append('BUY')\n",
    "        elif invested:\n",
    "            if row['high'] > take_profit:\n",
    "                invested = False\n",
    "                action.append('PROFIT')\n",
    "                stake.append(stake[-1] / prev_close * take_profit)\n",
    "            elif row['low'] < stop_loss:\n",
    "                invested = False\n",
    "                action.append('LOSS')\n",
    "                stake.append(stake[-1] / prev_close * stop_loss)\n",
    "                last_loss = i\n",
    "                frozen = True\n",
    "            else:\n",
    "                stake.append(stake[-1] / prev_close * row['close'])\n",
    "                action.append('----')\n",
    "        else:\n",
    "            stake.append(stake[-1])\n",
    "            action.append('----')\n",
    "        prev_close = row['close']\n",
    "    df_sim['stake'] = stake[1:]\n",
    "    df_sim['action'] = action\n",
    "    return df_sim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "democratic-deposit",
   "metadata": {},
   "outputs": [],
   "source": [
    "from hypecommons import *\n",
    "\n",
    "def soru_viga_ellipse(X, Y, decfun=False):\n",
    "    A = .012\n",
    "    B = .046\n",
    "    x = X - .998\n",
    "    y = Y - .995\n",
    "    alpha = -pi/14\n",
    "    F = (x * cos(alpha) + y * sin(alpha)) ** 2 / A ** 2 + (x * sin(alpha) - y * cos(alpha)) ** 2 / B ** 2 - 1\n",
    "    return F if decfun else F >= 0\n",
    "\n",
    "def run_simulation_on_data(df_sim, signal_function, target=1.02, stop=0.94, hrs_frozen=0, fees=0.001):\n",
    "    df_sim = normalise(df_sim)\n",
    "    df_sim.dropna(inplace=True)\n",
    "    df_sim['pred'] = np.vectorize(signal_function)(df_sim['close_ema26_norm'], df_sim['close_ma200_norm'])\n",
    "    df_sim = df_sim.set_index(pd.to_datetime(df_sim.index))\n",
    "\n",
    "    # ------- simulation -------\n",
    "    df_sim = simulate(df_sim, target=target, stop=stop, hrs_frozen=hrs_frozen, fees=fees)\n",
    "\n",
    "    display(df_sim[df_sim['action'].isin(['PROFIT', 'LOSS'])][\n",
    "        ['close', 'close_ema26_norm', 'close_ma200_norm', 'stake', 'action']])\n",
    "\n",
    "    df_sim[df_sim['action'].isin(['PROFIT', 'LOSS'])]['action'].str.get_dummies().sum().plot(\n",
    "        kind='pie', label='action', autopct='%1.0f%%', figsize=[5, 5])\n",
    "    plt.show()\n",
    "\n",
    "    plot(plt.plot, df_sim, ['stake'], fig_size=(16, 6))\n",
    "    plot(plt.plot, df_sim, ['close'], fig_size=(16, 6))\n",
    "    \n",
    "    return df_sim\n",
    "\n",
    "def run_simulation(symbol, start, days, signal_function, freq=1, target=1.02, stop=0.94, hrs_frozen=0, fees=0.001):\n",
    "    df_sim = download_history_fast(symbol, start, freq=freq, days=days)\n",
    "    return run_simulation_on_data(df_sim, signal_function, target=target, stop=stop, hrs_frozen=hrs_frozen, fees=fees)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "danish-sphere",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "_ = run_simulation('BNBBUSD', '20210601000000', 153, soru_viga_ellipse, freq=1, \n",
    "                   target=1.02, stop=0.94, hrs_frozen=0, fees=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "desirable-turkey",
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = run_simulation('BNBUSDT', '20171225000000', 365+7, soru_viga_ellipse, freq=1, \n",
    "                   target=1.02, stop=0.94, hrs_frozen=0, fees=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "genetic-effort",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sim = download_history_fast('BNBUSDT', '20171225000000', freq=1, days=365+7)\n",
    "\n",
    "df_sim.drop(df_sim[df_sim.index < datetime(2018, 8, 1)].index, inplace=True)\n",
    "\n",
    "run_simulation_on_data(df_sim, soru_viga_ellipse, target=1.01, stop=0.94, hrs_frozen=0, fees=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "tribal-malpractice",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sim = download_history_fast('BNBUSDT', '20171225000000', freq=1, days=365+7)\n",
    "\n",
    "df_sim = run_simulation_on_data(df_sim, soru_viga_ellipse)\n",
    "\n",
    "df_sim['close_stdev_norm'] = df_sim['close'].rolling(window=200).std() / df_sim['close'] * 100\n",
    "df_sim['close_stdev_ema200_norm'] = ema(df_sim['close_stdev_norm'], 20000)\n",
    "\n",
    "df_sim.dropna(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "gross-klein",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot(plt.plot, df_sim, ['close_stdev_ema200_norm'], fig_size=(16, 6), baseline=0.6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "constitutional-replication",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sim['stake_diff_norm'] = df_sim[::60*24]['stake'].diff()\n",
    "df_sim[df_sim['stake_diff_norm'].notna()][['stake_diff_norm']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "hybrid-visit",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot(plt.plot, df_sim[::60*24], ['stake_diff_norm'], fig_size=(16, 6))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "incredible-think",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
